{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607e5218-9d48-4d71-922a-b93d3e6378b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cad8b-99d6-432f-946c-848f24378e2d",
   "metadata": {},
   "source": [
    "# UNET Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db82d89-5200-4717-8e8f-573f0d30121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, init_features=32):\n",
    "        super().__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        return torch.sigmoid(self.conv(dec1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=features,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=features,\n",
    "                out_channels=features,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ccc07-94e2-4186-857b-225066b45fec",
   "metadata": {},
   "source": [
    "# LNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceafe290-e8c0-4243-a850-79134ddc0a7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLNet\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, init_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(LNet, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()  \u001b[38;5;66;03m# Corrected this line\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class LNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=16):\n",
    "        super(LNet, self).__init__()  # Corrected this line\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = self._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = self._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = self._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = self._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = self._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        return self.sigmoid(self.conv(dec1)) \n",
    "\n",
    "    def get_architecture_type(self):\n",
    "        return \"UNET\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=features),  \n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.Dropout(0.3)  \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd774716-d163-4dce-86ce-f65738eb30f5",
   "metadata": {},
   "source": [
    "# LNet with Reverse Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2ebd3-b0a7-4b0a-938a-ec0c44256dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ReverseAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.selu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.selu(self.bn1(self.conv1(x))) \n",
    "        attention = self.sigmoid(self.bn2(self.conv2(attention))) \n",
    "        return x * (1 - attention) + x \n",
    "\n",
    "class LNet_RA(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=16):\n",
    "        super(LNet_RA, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = self._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = self._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = self._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = self._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = self._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.reverse_attention = ReverseAttention(features)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        refined_output = self.reverse_attention(dec1)\n",
    "\n",
    "        return self.sigmoid(self.conv(refined_output))  \n",
    "\n",
    "    def get_architecture_type(self):\n",
    "        return \"UNET with Reverse Attention\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a4bfbc-415b-4085-9df4-41c6fda3732b",
   "metadata": {},
   "source": [
    "# Unet with reverse attention in each skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5ddf5-7fbc-4cea-acc0-9a3bae577f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNet_RA_SC(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, init_features=16):\n",
    "        super().__init__()\n",
    "        f = init_features\n",
    "        # ---------- Encoder ----------\n",
    "        self.enc1 = self._block(in_channels, f)\n",
    "        self.enc2 = self._block(f,   f*2)\n",
    "        self.enc3 = self._block(f*2, f*4)\n",
    "        self.enc4 = self._block(f*4, f*8)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # ---------- Bottleneck ----------\n",
    "        self.bottleneck = self._block(f*8, f*16)\n",
    "\n",
    "        # ---------- Decoder ----------\n",
    "        self.up4 = nn.ConvTranspose2d(f*16, f*8, 2, 2)\n",
    "        self.dec4 = self._block(f*16, f*8)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(f*8, f*4, 2, 2)\n",
    "        self.dec3 = self._block(f*8, f*4)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(f*4, f*2, 2, 2)\n",
    "        self.dec2 = self._block(f*4, f*2)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(f*2, f, 2, 2)\n",
    "        self.dec1 = self._block(f*2, f)\n",
    "\n",
    "        # ---------- Reverseâ€‘Attention para cada skip ----------\n",
    "        self.ra1 = ReverseAttention(f)\n",
    "        self.ra2 = ReverseAttention(f*2)\n",
    "        self.ra3 = ReverseAttention(f*4)\n",
    "        self.ra4 = ReverseAttention(f*8)\n",
    "\n",
    "        # ---------- Head ----------\n",
    "        self.head = nn.Conv2d(f, out_channels, 1)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)              # f\n",
    "        e2 = self.enc2(self.pool(e1))  # 2f\n",
    "        e3 = self.enc3(self.pool(e2))  # 4f\n",
    "        e4 = self.enc4(self.pool(e3))  # 8f\n",
    "\n",
    "        b  = self.bottleneck(self.pool(e4))  # 16f\n",
    "\n",
    "        d4 = self.up4(b)\n",
    "        d4 = torch.cat([d4, self.ra4(e4)], 1)\n",
    "        d4 = self.dec4(d4)\n",
    "\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat([d3, self.ra3(e3)], 1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, self.ra2(e2)], 1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, self.ra1(e1)], 1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        return torch.sigmoid(self.head(d1))\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _block(in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_c, out_c, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ffc1a-db54-4182-856d-9674f8c4cca1",
   "metadata": {},
   "source": [
    "# LNet With Layer NOrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2f739-3061-434d-be26-8bcb8a9837a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNet_LN(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=16, activation=nn.GELU, use_layer_norm=True):\n",
    "        super(LNet_LN, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        \n",
    "        features = init_features\n",
    "        self.encoder1 = self._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = self._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = self._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = self._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = self._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        return self.sigmoid(self.conv(dec1))\n",
    "\n",
    "    def get_architecture_type(self):\n",
    "        return \"UNET\"\n",
    "\n",
    "    def _block(self, in_channels, features, name):\n",
    "        norm_layer = nn.LayerNorm([features, 512, 512]) if self.use_layer_norm else nn.BatchNorm2d(features)\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, kernel_size=3, padding=1, bias=False),\n",
    "            norm_layer,\n",
    "            self.activation(),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n",
    "            norm_layer,\n",
    "            self.activation(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570cc0af-0b92-4f5f-8be2-ffd4b361043e",
   "metadata": {},
   "source": [
    "# LNET WITH Multi-Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef2052-d5a8-4e05-80b6-2d96214d9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNet_MS(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=16):\n",
    "        super(LNet_MS, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = self._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = self._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = self._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = self._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = self._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.reverse_attention = ReverseAttention(features)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        refined_output = self.reverse_attention(dec1)\n",
    "\n",
    "        return self.sigmoid(self.conv(refined_output))  \n",
    "\n",
    "    def get_architecture_type(self):\n",
    "        return \"UNET with Reverse Attention and Multi-Scale Features\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f9f1b-5b74-42fc-bd92-d4915ca1a015",
   "metadata": {},
   "source": [
    "# LNET with DIlated Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09728a-d0bf-4758-b3ab-7b24dbb6c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNet_RA_Dilated(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=16):\n",
    "        super(LNet_RA_Dilated, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = self._block(in_channels, features, dilation=1, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = self._block(features, features * 2, dilation=1, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = self._block(features * 2, features * 4, dilation=2, name=\"enc3\")  \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = self._block(features * 4, features * 8, dilation=2, name=\"enc4\") \n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = self._block(features * 8, features * 16, dilation=4, name=\"bottleneck\")  \n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self._block((features * 8) * 2, features * 8, dilation=1, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self._block((features * 4) * 2, features * 4, dilation=1, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self._block((features * 2) * 2, features * 2, dilation=1, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self._block(features * 2, features, dilation=1, name=\"dec1\")\n",
    "\n",
    "        self.reverse_attention = ReverseAttention(features)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()  \n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        refined_output = self.reverse_attention(dec1)\n",
    "\n",
    "        return self.sigmoid(self.conv(refined_output)) \n",
    "\n",
    "    def get_architecture_type(self):\n",
    "        return \"UNET with Reverse Attention and Dilated Convolutions\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, dilation=1, name=\"block\"):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, kernel_size=3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
